{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (EX) News article processing (with ML pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yr9_JzZwQ6"
   },
   "source": [
    "# `agnews` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S51T-4qYZ0LU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 29.3M    0  5512    0     0   5973      0  1:25:51 --:--:--  1:25:51  5991\n",
      "  0 29.3M    0  208k    0     0   105k      0  0:04:45  0:00:01  0:04:44  105k\n",
      "  2 29.3M    2  656k    0     0   228k      0  0:02:11  0:00:02  0:02:09  229k\n",
      "  3 29.3M    3  944k    0     0   244k      0  0:02:03  0:00:03  0:02:00  244k\n",
      "  3 29.3M    3 1150k    0     0   226k      0  0:02:12  0:00:05  0:02:07  226k\n",
      "  4 29.3M    4 1326k    0     0   225k      0  0:02:13  0:00:05  0:02:08  266k\n",
      "  5 29.3M    5 1662k    0     0   241k      0  0:02:04  0:00:06  0:01:58  296k\n",
      "  6 29.3M    6 1838k    0     0   233k      0  0:02:08  0:00:07  0:02:01  236k\n",
      "  7 29.3M    7 2158k    0     0   240k      0  0:02:05  0:00:08  0:01:57  237k\n",
      "  8 29.3M    8 2446k    0     0   247k      0  0:02:01  0:00:09  0:01:52  270k\n",
      "  9 29.3M    9 2734k    0     0   250k      0  0:02:00  0:00:10  0:01:50  278k\n",
      "  9 29.3M    9 2958k    0     0   249k      0  0:02:00  0:00:11  0:01:49  259k\n",
      " 10 29.3M   10 3259k    0     0   253k      0  0:01:58  0:00:12  0:01:46  283k\n",
      " 11 29.3M   11 3579k    0     0   255k      0  0:01:57  0:00:14  0:01:43  282k\n",
      " 12 29.3M   12 3787k    0     0   254k      0  0:01:58  0:00:14  0:01:44  267k\n",
      " 13 29.3M   13 4123k    0     0   259k      0  0:01:55  0:00:15  0:01:40  281k\n",
      " 14 29.3M   14 4491k    0     0   266k      0  0:01:52  0:00:16  0:01:36  306k\n",
      " 15 29.3M   15 4667k    0     0   260k      0  0:01:55  0:00:17  0:01:38  280k\n",
      " 16 29.3M   16 4923k    0     0   260k      0  0:01:55  0:00:18  0:01:37  276k\n",
      " 17 29.3M   17 5307k    0     0   265k      0  0:01:53  0:00:19  0:01:34  297k\n",
      " 18 29.3M   18 5705k    0     0   273k      0  0:01:49  0:00:20  0:01:29  316k\n",
      " 20 29.3M   20 6025k    0     0   275k      0  0:01:49  0:00:21  0:01:28  306k\n",
      " 20 29.3M   20 6249k    0     0   273k      0  0:01:49  0:00:22  0:01:27  317k\n",
      " 21 29.3M   21 6489k    0     0   271k      0  0:01:50  0:00:23  0:01:27  311k\n",
      " 22 29.3M   22 6665k    0     0   268k      0  0:01:52  0:00:24  0:01:28  278k\n",
      " 23 29.3M   23 7145k    0     0   276k      0  0:01:48  0:00:25  0:01:23  288k\n",
      " 25 29.3M   25 7753k    0     0   288k      0  0:01:44  0:00:26  0:01:18  345k\n",
      " 27 29.3M   27 8229k    0     0   294k      0  0:01:41  0:00:27  0:01:14  394k\n",
      " 28 29.3M   28 8629k    0     0   298k      0  0:01:40  0:00:28  0:01:12  428k\n",
      " 29 29.3M   29 8917k    0     0   298k      0  0:01:40  0:00:29  0:01:11  447k\n",
      " 29 29.3M   29 9013k    0     0   291k      0  0:01:43  0:00:30  0:01:13  370k\n",
      " 30 29.3M   30 9172k    0     0   286k      0  0:01:44  0:00:32  0:01:12  274k\n",
      " 31 29.3M   31 9428k    0     0   286k      0  0:01:44  0:00:32  0:01:12  240k\n",
      " 32 29.3M   32 9748k    0     0   287k      0  0:01:44  0:00:33  0:01:11  223k\n",
      " 33 29.3M   33  9.9M    0     0   291k      0  0:01:42  0:00:34  0:01:08  254k\n",
      " 35 29.3M   35 10.4M    0     0   298k      0  0:01:40  0:00:35  0:01:05  342k\n",
      " 36 29.3M   36 10.7M    0     0   298k      0  0:01:40  0:00:36  0:01:04  378k\n",
      " 37 29.3M   37 11.0M    0     0   299k      0  0:01:40  0:00:37  0:01:03  384k\n",
      " 39 29.3M   39 11.4M    0     0   302k      0  0:01:39  0:00:38  0:01:01  403k\n",
      " 40 29.3M   40 11.8M    0     0   303k      0  0:01:38  0:00:39  0:00:59  386k\n",
      " 42 29.3M   42 12.3M    0     0   310k      0  0:01:36  0:00:40  0:00:56  392k\n",
      " 43 29.3M   43 12.7M    0     0   311k      0  0:01:36  0:00:41  0:00:55  407k\n",
      " 44 29.3M   44 13.0M    0     0   311k      0  0:01:36  0:00:42  0:00:54  399k\n",
      " 45 29.3M   45 13.2M    0     0   307k      0  0:01:37  0:00:44  0:00:53  343k\n",
      " 45 29.3M   45 13.3M    0     0   303k      0  0:01:38  0:00:45  0:00:53  303k\n",
      " 46 29.3M   46 13.7M    0     0   305k      0  0:01:38  0:00:46  0:00:52  273k\n",
      " 47 29.3M   47 13.9M    0     0   305k      0  0:01:38  0:00:46  0:00:52  256k\n",
      " 48 29.3M   48 14.2M    0     0   304k      0  0:01:38  0:00:47  0:00:51  246k\n",
      " 50 29.3M   50 14.6M    0     0   307k      0  0:01:37  0:00:48  0:00:49  309k\n",
      " 51 29.3M   51 14.9M    0     0   307k      0  0:01:37  0:00:49  0:00:48  347k\n",
      " 52 29.3M   52 15.5M    0     0   312k      0  0:01:36  0:00:50  0:00:46  373k\n",
      " 53 29.3M   53 15.8M    0     0   312k      0  0:01:36  0:00:51  0:00:45  380k\n",
      " 54 29.3M   54 15.9M    0     0   309k      0  0:01:37  0:00:52  0:00:45  352k\n",
      " 55 29.3M   55 16.3M    0     0   311k      0  0:01:36  0:00:53  0:00:43  350k\n",
      " 56 29.3M   56 16.6M    0     0   310k      0  0:01:36  0:00:54  0:00:42  341k\n",
      " 57 29.3M   57 16.9M    0     0   310k      0  0:01:36  0:00:55  0:00:41  295k\n",
      " 58 29.3M   58 17.2M    0     0   311k      0  0:01:36  0:00:56  0:00:40  297k\n",
      " 60 29.3M   60 17.6M    0     0   313k      0  0:01:35  0:00:57  0:00:38  355k\n",
      " 61 29.3M   61 17.9M    0     0   312k      0  0:01:36  0:00:58  0:00:38  320k\n",
      " 62 29.3M   62 18.4M    0     0   316k      0  0:01:35  0:00:59  0:00:36  373k\n",
      " 64 29.3M   64 19.0M    0     0   320k      0  0:01:33  0:01:00  0:00:33  425k\n",
      " 66 29.3M   66 19.5M    0     0   322k      0  0:01:33  0:01:01  0:00:32  452k\n",
      " 67 29.3M   67 19.7M    0     0   321k      0  0:01:33  0:01:02  0:00:31  420k\n",
      " 68 29.3M   68 20.1M    0     0   323k      0  0:01:32  0:01:03  0:00:29  450k\n",
      " 69 29.3M   69 20.3M    0     0   320k      0  0:01:33  0:01:04  0:00:29  374k\n",
      " 71 29.3M   71 20.9M    0     0   325k      0  0:01:32  0:01:05  0:00:27  396k\n",
      " 73 29.3M   73 21.5M    0     0   330k      0  0:01:31  0:01:06  0:00:25  422k\n",
      " 75 29.3M   75 22.2M    0     0   335k      0  0:01:29  0:01:08  0:00:21  501k\n",
      " 76 29.3M   76 22.4M    0     0   333k      0  0:01:30  0:01:08  0:00:22  469k\n",
      " 78 29.3M   78 23.1M    0     0   338k      0  0:01:28  0:01:09  0:00:19  577k\n",
      " 79 29.3M   79 23.3M    0     0   335k      0  0:01:29  0:01:11  0:00:18  454k\n",
      " 81 29.3M   81 23.7M    0     0   338k      0  0:01:28  0:01:11  0:00:17  453k\n",
      " 82 29.3M   82 24.3M    0     0   342k      0  0:01:27  0:01:12  0:00:15  437k\n",
      " 84 29.3M   84 24.8M    0     0   343k      0  0:01:27  0:01:13  0:00:14  482k\n",
      " 85 29.3M   85 25.0M    0     0   342k      0  0:01:27  0:01:14  0:00:13  392k\n",
      " 87 29.3M   87 25.5M    0     0   345k      0  0:01:26  0:01:15  0:00:11  496k\n",
      " 89 29.3M   89 26.2M    0     0   349k      0  0:01:25  0:01:16  0:00:09  504k\n",
      " 90 29.3M   90 26.4M    0     0   347k      0  0:01:26  0:01:17  0:00:09  423k\n",
      " 91 29.3M   91 26.8M    0     0   348k      0  0:01:26  0:01:18  0:00:08  414k\n",
      " 93 29.3M   93 27.4M    0     0   351k      0  0:01:25  0:01:20  0:00:05  476k\n",
      " 95 29.3M   95 27.9M    0     0   353k      0  0:01:24  0:01:20  0:00:04  479k\n",
      " 96 29.3M   96 28.3M    0     0   354k      0  0:01:24  0:01:21  0:00:03  431k\n",
      " 98 29.3M   98 28.9M    0     0   356k      0  0:01:24  0:01:23  0:00:01  504k\n",
      " 99 29.3M   99 29.1M    0     0   356k      0  0:01:24  0:01:23  0:00:01  476k\n",
      " 99 29.3M   99 29.2M    0     0   353k      0  0:01:25  0:01:24  0:00:01  385k\n",
      "100 29.3M  100 29.3M    0     0   353k      0  0:01:25  0:01:25 --:--:--  345k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews.csv -O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WOEde4yY8_u"
   },
   "source": [
    "# Pipelining with PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rWgYKCtGY8vy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PC9bF4RLQvsn"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xhcOjvnyqOUf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|Class Index|               Title|         Description|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|Wall St. Bears Cl...|Reuters - Short-s...|\n",
      "|          3|Carlyle Looks Tow...|Reuters - Private...|\n",
      "|          3|Oil and Economy C...|Reuters - Soaring...|\n",
      "|          3|Iraq Halts Oil Ex...|Reuters - Authori...|\n",
      "|          3|Oil prices soar t...|AFP - Tearaway wo...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGqkLxIDt8L8"
   },
   "source": [
    "# Arrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8jgV-jK9snHr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|               Title|         Description|                text|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    3|Wall St. Bears Cl...|Reuters - Short-s...|Wall St. Bears Cl...|\n",
      "|    3|Carlyle Looks Tow...|Reuters - Private...|Carlyle Looks Tow...|\n",
      "|    3|Oil and Economy C...|Reuters - Soaring...|Oil and Economy C...|\n",
      "|    3|Iraq Halts Oil Ex...|Reuters - Authori...|Iraq Halts Oil Ex...|\n",
      "|    3|Oil prices soar t...|AFP - Tearaway wo...|Oil prices soar t...|\n",
      "|    3|Stocks End Up, Bu...|Reuters - Stocks ...|Stocks End Up, Bu...|\n",
      "|    3|Money Funds Fell ...|AP - Assets of th...|Money Funds Fell ...|\n",
      "|    3|Fed minutes show ...|USATODAY.com - Re...|Fed minutes show ...|\n",
      "|    3|Safety Net (Forbe...|\"Forbes.com - Aft...|Safety Net (Forbe...|\n",
      "|    3|Wall St. Bears Cl...| NEW YORK (Reuter...|Wall St. Bears Cl...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
    "\n",
    "# renaming 'Class Index' col to 'label'\n",
    "df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "# add a column with concatenated text called 'text\n",
    "df = df.withColumn('text', concat_ws(' ', col('Title'), col('Description')))\n",
    "\n",
    "# concatenating texts\n",
    "# df = df.select('label', 'text')\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4s8B9lDuSZm"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iluZMjpnuNnT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               Title|         Description|                text|               words|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3|Wall St. Bears Cl...|Reuters - Short-s...|Wall St. Bears Cl...|[wall, st, bears,...|\n",
      "|    3|Carlyle Looks Tow...|Reuters - Private...|Carlyle Looks Tow...|[carlyle, looks, ...|\n",
      "|    3|Oil and Economy C...|Reuters - Soaring...|Oil and Economy C...|[oil, and, econom...|\n",
      "|    3|Iraq Halts Oil Ex...|Reuters - Authori...|Iraq Halts Oil Ex...|[iraq, halts, oil...|\n",
      "|    3|Oil prices soar t...|AFP - Tearaway wo...|Oil prices soar t...|[oil, prices, soa...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "\n",
    "# convert sentences to list of words\n",
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='words', pattern='\\\\W')\n",
    "\n",
    "# applies tokenizer\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zujVzF9vGQ4"
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jwqRSHqtu12B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               Title|         Description|                text|               words|            filtered|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3|Wall St. Bears Cl...|Reuters - Short-s...|Wall St. Bears Cl...|[wall, st, bears,...|[wall, st, bears,...|\n",
      "|    3|Carlyle Looks Tow...|Reuters - Private...|Carlyle Looks Tow...|[carlyle, looks, ...|[carlyle, looks, ...|\n",
      "|    3|Oil and Economy C...|Reuters - Soaring...|Oil and Economy C...|[oil, and, econom...|[oil, economy, cl...|\n",
      "|    3|Iraq Halts Oil Ex...|Reuters - Authori...|Iraq Halts Oil Ex...|[iraq, halts, oil...|[iraq, halts, oil...|\n",
      "|    3|Oil prices soar t...|AFP - Tearaway wo...|Oil prices soar t...|[oil, prices, soa...|[oil, prices, soa...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lB7I_H3vSKJ"
   },
   "source": [
    "# Term frequency, Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3vfAqoeZvRwV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               Title|         Description|                text|               words|            filtered|        raw_features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3|Wall St. Bears Cl...|Reuters - Short-s...|Wall St. Bears Cl...|[wall, st, bears,...|[wall, st, bears,...|(16384,[906,1198,...|\n",
      "|    3|Carlyle Looks Tow...|Reuters - Private...|Carlyle Looks Tow...|[carlyle, looks, ...|[carlyle, looks, ...|(16384,[98,156,14...|\n",
      "|    3|Oil and Economy C...|Reuters - Soaring...|Oil and Economy C...|[oil, and, econom...|[oil, economy, cl...|(16384,[338,1612,...|\n",
      "|    3|Iraq Halts Oil Ex...|Reuters - Authori...|Iraq Halts Oil Ex...|[iraq, halts, oil...|[iraq, halts, oil...|(16384,[180,2731,...|\n",
      "|    3|Oil prices soar t...|AFP - Tearaway wo...|Oil prices soar t...|[oil, prices, soa...|[oil, prices, soa...|(16384,[1546,1752...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# calculate term frequency in each article (row)\n",
    "hashing_tf = HashingTF(inputCol='filtered', outputCol='raw_features', numFeatures=16384)  \n",
    "\n",
    "# 16384 --> in BERT we represent one item with 780 features\n",
    "# compressing text into 780 numbers\n",
    "# within all possible words kept, only 16384 words are kept\n",
    "# 2^14 === 16384\n",
    "\n",
    "# applie term frequency\n",
    "featured_data = hashing_tf.transform(df)\n",
    "\n",
    "featured_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C0bYYaoZvihR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               Title|         Description|                text|               words|            filtered|        raw_features|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3|Wall St. Bears Cl...|Reuters - Short-s...|Wall St. Bears Cl...|[wall, st, bears,...|[wall, st, bears,...|(16384,[906,1198,...|(16384,[906,1198,...|\n",
      "|    3|Carlyle Looks Tow...|Reuters - Private...|Carlyle Looks Tow...|[carlyle, looks, ...|[carlyle, looks, ...|(16384,[98,156,14...|(16384,[98,156,14...|\n",
      "|    3|Oil and Economy C...|Reuters - Soaring...|Oil and Economy C...|[oil, and, econom...|[oil, economy, cl...|(16384,[338,1612,...|(16384,[338,1612,...|\n",
      "|    3|Iraq Halts Oil Ex...|Reuters - Authori...|Iraq Halts Oil Ex...|[iraq, halts, oil...|[iraq, halts, oil...|(16384,[180,2731,...|(16384,[180,2731,...|\n",
      "|    3|Oil prices soar t...|AFP - Tearaway wo...|Oil prices soar t...|[oil, prices, soa...|[oil, prices, soa...|(16384,[1546,1752...|(16384,[1546,1752...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# inverse document frequency\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "\n",
    "# applies IDF\n",
    "idf_vectorizer = idf.fit(featured_data)\n",
    "\n",
    "rescaled_data = idf_vectorizer.transform(featured_data)\n",
    "\n",
    "rescaled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AwXEZpVswcO_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|raw_features                                                                                                                                                                                                                                  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(16384,[906,1198,4756,5540,5638,5831,6235,7372,8905,11170,11790,12343,12766,13441,14118,16126],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                             |\n",
      "|(16384,[98,156,1445,1913,2309,2586,2614,2615,3244,4198,4510,5278,6187,6701,7622,8145,8346,8804,9604,9990,10122,10300,11170,14429,14587],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(16384,[906,1198,4756,5540,5638,5831,6235,7372,8905,11170,11790,12343,12766,13441,14118,16126],[8.537787661984513,3.397294380630043,4.39465293559298,9.153187225099833,6.786850187276713,7.767679440288439,7.112272587711341,5.227244648590488,6.052881012196512,4.445293364402747,7.314012230362397,4.99740821618902,4.572792771790261,7.056183121060297,6.158241527854338,4.98702151000021])                                                                                                                                                                                                                 |\n",
      "|(16384,[98,156,1445,1913,2309,2586,2614,2615,3244,4198,4510,5278,6187,6701,7622,8145,8346,8804,9604,9990,10122,10300,11170,14429,14587],[4.034428742143106,6.912476400394122,15.358252085893987,6.530916813139512,4.466370604406117,5.120716931166067,6.104174306584063,4.717879945464172,5.560219359058193,6.766230900073976,8.578609656504767,4.07141987887688,5.96670331595546,4.578881070657516,3.9018943033535805,3.6140183009099185,4.936647122178583,4.324179678935594,6.851388708414284,3.3098927597330214,7.662318924630612,5.192807960320586,4.445293364402747,4.758153844602112,3.8660809521961768])|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select('raw_features').show(2, truncate=False)\n",
    "rescaled_data.select('features').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "macNK1smxX5s"
   },
   "source": [
    "# Training a multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OeLPS8HwxXXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 95811\n",
      "Test Dataset Count: 31789\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing\n",
    "(train, test) = rescaled_data.randomSplit([0.75, 0.25])\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "01Ml9cUlww1m"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family='multinomial',\n",
    "                        regParam=0,\n",
    "                        maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Output column words already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m pipeline = Pipeline(stages=[tokenizer,\n\u001b[32m      2\u001b[39m                             stopwords_remover,\n\u001b[32m      3\u001b[39m                             hashing_tf,\n\u001b[32m      4\u001b[39m                             idf,\n\u001b[32m      5\u001b[39m                             lr])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m pipelineFit = pipeline.fit(df)\n\u001b[32m      9\u001b[39m dataset = pipelineFit.transform(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(dataset)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n\u001b[32m    133\u001b[39m     transformers.append(stage)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     dataset = stage.transform(dataset)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m    136\u001b[39m     model = stage.fit(dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\pyspark\\ml\\base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform(dataset)\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\pyspark\\ml\\util.py:270\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ConnectDataFrame(plan=plan, session=session)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:435\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m._java_obj.transform(dataset._jdf), dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0-Kathy Cui\\0-Northwestern\\0-Classes\\3rd Year\\Spring 2025\\CS352\\.conda\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Output column words already exists."
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords_remover,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(df)\n",
    "\n",
    "dataset = pipelineFit.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpkPVcdnz0Mi"
   },
   "source": [
    "# Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t880vBmqzswD"
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaiuTIx53cyq"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# labels = [\"World\", \"Sports\", \"Business\",\"Science\"]\n",
    "\n",
    "# take only the predictions\n",
    "preds_and_labels = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNtcwxpY4REC"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjK_PlwD4fqi"
   },
   "source": [
    "# Pipelining, from start to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYlA_fev4iV4"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
    "\n",
    "def arrangeColumns(df):\n",
    "  # Renaming 'Class Index' col to 'label'\n",
    "  df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "  # Add a new column 'text' by joining 'Title' and 'Description'\n",
    "  df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "  # Select new text feature and labels\n",
    "  df = df.select('label', 'text')\n",
    "  return df\n",
    "\n",
    "df = arrangeColumns(df)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# term frequency\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9_LHoAR5Cc-"
   },
   "outputs": [],
   "source": [
    "# Put everything in pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords_remover,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "\n",
    "# transform and train\n",
    "dataset = pipelineFit.transform(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAVBCKK/HPnCbpIGlAzRAR",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
